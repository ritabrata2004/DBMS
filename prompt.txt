Scan the codebase thoroughly and understand deeply how everythng works. Then write a report on it, explaining every functionality and how it is implemented in detail. Cover th following points :

Title, Group Name, Roll, Names
Problem Statement
Methodology
Results and/or Screenshot of the Demonstration
Conclusion and Future Scope
References (to sources you have used)
Title : Web Application to convert User input to SQL and run it on databases
Group Name : Lazy_GCD
Names : Aritra Maji(22CS30011), Sayandeep Bhowmick(22CS10069), Agniva Saha(22CS10003), Ritabrata Bharati(22CS10059), Raaja Das(22CS30043)
Problem Statement : The goal of this project is to build a LLM for generating SQL queries from natural language questions. User asks questions in a natural
language and the system generates answers by converting those
questions to an SQL query and then executing that query on
PostgreSQL database. Any open source LLM may be used.
Discuss every functionality as elaborately as you possibly can. Make it around 500 lines. Give it in multiple goes if required. Understand how the t Google login and normal login is configured to work together seamlessly, and how the configuring is done in the codebase.
Make the report as tech-savvy, industry-standard looking and professional as possible. Format it nicely. Don't write code snippets, explain in english. Write a nicely formatted markdown report. Be careful to write only stuff that are in the codebase, and stick to the codebase while preparing the report. Don't write anything that is different from the codebase or not in the codebase.
Start writing the report only after you understand the codebase thoroughly. Write the report in description.md.

Here's a discussion of the database.
The frontend is designed with JavaScript, TailwindCSS, React, framer-motion(for animations). It is a sleek, dark mode UI designed to be as user-friendly as possible. 
The application has 3 main pages - Databases, Metadata Manager, Dashboard. 
The Database page allows an user to add a new database. if the user provides the wrong credentials while adding a database, the application gives an error. It also displays the list of databases already added, from where you can select one and choose to either manage it metadata or query the database. When an user wants to query a database, he is directed to a query session - where he can enter natural language queries. The LLM takes them as input and generates SQL queries, and displays the generated query to the user, which the user cna choose to run on the database. The rows are returned to the user, and the user can either copy them or download them as a .xlsx file.
The Metadata Manager page allows an user to view a list of databases already added. The user can test the database connection for those databases, or select one of them. After selecting one, allows an user to extract metadata related to the database, using inbuilt PostgreSQL queries. It the displays the schema as tables, using different colours and shades to distingush distinct rows and columns. One can also view the relationships and view an auto-generated ER diagram. The user can also write an SQL query himself and run it on the database. 
The dashboard displays various statistics related to the user.

The application allows the user to either sign in using email, username and password or authenticate via Google accounts. The authentication is managed by Firebase. Also an user can change his password anytime he wants.

• Authentication (Firebase + react form) 
• Password change through email OTP 
• Database connector 
• Metadata extractor (which generates description automatically) 
• AI/human-based metadata description update 
• Schema and relationship page 
• ER Diagram page 
• Search metadata 
• SQL Query Executor 
• Session-based SQL query chatbot 
• Excel and JSON responses (with download option) 
• Dashboard with detailed statistics - generation time, query activity 
• User token usage data 
• RAG1 on table metadata to handle large data efficiently 
• RAG2 for in-content learning 
• Tech stack 
	1. Frontend 
	2. Backend 
	3. RAG - GPU intensive, we use streaming RAG on JSON dataset and metadata. We run this on a remote GPU server and connect to our application with ssh tunneling.

